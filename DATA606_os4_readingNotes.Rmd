---
title: "DATA606_os4_readings"
author: "Bonnie Cooper"
date: "2/8/2020"
output:
  html_document:
    #css: ./lab.css
    highlight: pygments
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br><br>

## OpenIntro Statistics Reading Notes
<br><br>

```{r, echo = FALSE}
library( tidyverse )
library( dplyr )
library( ggplot2 )
```


### Chapter 2: Summarizing Data
<br>
This Chapter focuses on the mechanics of generating summary statistics and summary graphs of data in R
<br>

#### 2.1 Examining Numerical Data
<br>

summarizing numerical variables using the loan50 and county datasets

```{r}
#read the .csv files from URLs hosted on github into R dataframes 
countyURL <- 'https://raw.githubusercontent.com/SmilodonCub/DATA606/master/county.csv'
loan50URL <- 'https://raw.githubusercontent.com/SmilodonCub/DATA606/master/loan50.csv'
county <- read.csv( url( countyURL ) )
loan50 <- read.csv( url( loan50URL ) )
```

##### Scatterplots for paired data

case-by-case view of data for numerical variables

```{r}
data2plot <- loan50 %>% select( Income = total_income, Loan = loan_amount )
ggplot(data2plot, aes(x=Income, y=Loan)) +
    geom_point(shape=1) +    # Use hollow circles
    geom_smooth()            # Add a loess smoothed fit curve with confidence region
#> `geom_smooth()` using method = 'loess'
```

The relationship between variables in these two graphs is NONLINEAR: shows relationships best described with curvature rather than a straight line.

```{r}
data2plot <- county %>% select( Pov_Rate = poverty, Income = median_hh_income )
ggplot(data2plot, aes(x=Pov_Rate, y=Income)) +
    geom_point(shape=1) +    # Use hollow circles
    geom_smooth()            # Add a loess smoothed fit curve with confidence region
#> `geom_smooth()` using method = 'loess'
```

#### Dot plots for single variables.
Mean is a common ay to measure the center of a distribution
Sum(values)/num(values)
the sample mean can be computed as the sum of the observed values divided by the number of observations.
mu represents the average of all observations in the population; the sample mean provides a rough estimate of mu. the sample mean is a point estimate; the larger the sample, the better the estimate.


### Chapter 3: Probability  

#### Probability

**Probability:** the probability of an outcome is the proportion of times the outcome would occur if we observed the random process an infinite number of times. It always takes values between 0 and 1.

**Law of Large Numbers:** The tendancy for the proportion of outcomes to converge on a mean proportion as the number of recorded events increases. As more observations are collected, the proportion p_hat of occurrences with a particular outcome converges to the probability p of that outome

**Disjointed, or mutually exclusive events** are events that cannot both happem simultaneously. e.g. a die roll cannot simultaneously be '1' and '0'

**Addition Rules**

* **Addition Rule of Disjoint Outcomes:** if A1 and A2 represent two disjoint outcomes, then the probability of either event happening is found by adding the individual event probabilities: P(A1 or A2) = P(A1)+P(A2)
* **General Addition Rule** if A and B are any two events, disjoint or not, then the probability that at least one of them will occur is: P( A or B )= P(A)+P(B)-P(A and B)

**Probability Distribution** a list of all disjoint probability outcomes and their associated probabilities such that (1) the outcomes are disjoint (2)each probability is between 0 and 1 (3) the sum of probabilities is 1. Very convenient to visualize as a bar plot such that the bar heights = event probabilities

**Sample Space** the set of all possible outcomes

**Complement** the complement of A, A_c represents all the outcomes in the sample space not A such that P(A) + P(A_c) = 1

**Independence** two processes can be independant if knowing the outcome of one provides no useful information about the outcome of the other.

**Multiplication Rule for Independent Processes:** If A and B represent events from two different and independent processes, then the probability that both A and B occur can be calculated as the product of their seperate probabilities: P(A+B)=P(A)*P(B)

#### Conditional Probability

**Marginal Probability** probabiity based on a single variable without regard to any other variables P(A) or P(B)

**Joint Probability** probabiity of outcomes for two or more variables or processes. P(A & B)

**Conditional Probability** There are two parts to a conditional probability, the outcome of interest and the condition. the condition is information we know to be true and the outcome of interest can be described as a known outcome or event. P(A|B). The conditional probability of outcome A given condition B is computed as following: P(A|B)=P(A&B)/P(B)

**General Multiplicative Rule** If A and B represent two outcomes or events, then P(A&B)=P(A|B)*P(B). Itis useful to think of A as the outcome of interest and B as the condition.
Example: P(result=lived & inoculated=no) given that 
P(result=lived | inoculated=no)=0.8588
P(inoculated=no)=0.9608

**Sum of Conditional Probabilities** Let A1,...,Ak represent all the disjoint outcomes for a variable or process. Then if B is an event, possibly for another variable or process, we have P(A1|B)+...+P(Ak|B)=1. The rule for complements also holds when an event and its complement are conditioned on the same information: P(A|B)=1-P(Acomp|B).

**Bayes' Theorem: Inverting Probabilities:** consider the following conditional probability for variable 1 & variable 2:
P(outcome A1 of V1 | outcome B of variable 2)
Bayes' Theorem states that this conditional probabiity can be identified as the following fraction:
P(B|A1)P(A1)/(P(B|A1)P(A1)+P(B|A2)P(A2)+...+P(B|Ak)P(Ak))
where A2, A3, ..., Ak represent all other possible outcomes of the first variable.
Basically, the numerator identifies the probability of getting both A1 & B. The denminator is the marginal probability of getting B.
To apply Bayes' Theorem correctly, there are two preparatory steps:

1. First identify the marginal probabilities of each possible outcome of the first variable: P(A1), P(A2), ...,P(Ak)
2. The identify the probability of the outcome B, conditioned on each possible scenario for the first variable: P(B|A1), P(B|A2),...,P(B|Ak)
Once each of these probabilities are identified, they can be applied directly within Bayes' formula.

**Bayesian Statistics** applying Bayes' theorem allows us to update our belief about whether an event occured given a condition. updating beliefs using Bayes' Theorem is the foundation of Bayesian Statistics.

**Replacement** If we sample from a small population without replacement, we no longer have independence between our observations. Sampling with replacement keeps events independent.

**Random Variable** A random process or variable with a numerical outcome

**Expected Value** The expected value for a random variable represents the average outcome. If a random variable, X, takes outcomes x1, x2,...,xk with probabilities P(X=x1),...,P(X=xk) the expected value of X is the sum of each outcome multiplied by its corresponding probability:
E(X)=x1 x P(X=x1)+...+xk x P(X=xk) = sum(xi x P(X=xi)) = mu

**General Variance Formula** If X takes outcomes x1,...xk with probabilities P(X=x1),...,P(X=xk) and expected value mu=E(X), then the variance of X, denoted by Var(X) or sigma^2, is
sigma^2 = (x1-mu)^2 x P(X=x1)+...+(xk-mu)^2 x P(X=xk)
The standard deviationof X labeled sigma is the sqrt of the variance

**Linear Combinations** a fancy way to describe aX+bY where a and b are some fixed numbers. To compute the average value of a linear combination of random variables, plug in the average of each individual random variable and compute the result: aE(X)+bE(x). Recall that the expected value is the same as the mean,E(X)=mu.

**Variability in Linear Combinations of Random Variables** understanding the uncertainty associated with the total outcome of the combination of random variables. The vaiance of a linear combination of random variables may be computed by squaring the constants, substituting in the variances for the random variables, and computing the result:
Var(aX+bY)=a^2Var(X) + b^2Var(Y)
This is valid as long as the random variables are independent. 

**Probability Density Function** area under the curve = 1


## Chapter 4 
## Distributions of Random Variables

### Normal Distribution
symmetric, unimodal, bell curve. many variables are nearly normal but none are exactly normal. Id a normal distribution had mean mu and standard deviation sigma, we may write the distribution as N( mu,sigma )

Standardizing with Zscores. put data onto a standardized scale which can make comparisons more reasonable.
**Zscore** of an observation is defined as the number of standard deviations it falls above or below a mean. Observations above hte mean always have posiive Zscores, while those below the mean always have negative Zscores.  Z = ( x-mu )/sigma
An observation is said to be more unusual than another observation if the absolute value of the Zscore is greater. |Z1|>|Z2|

**Tail Areas** or percentile. use pnorm() function. Solve normal distribution problems by finding the Zscore
**Always draw a picture first, and find the Zscore second** for any normal probability situation, always always always draw and label the normal curve and shade the area of interest first. the picture will provide an estimate of the probability. After drawing a figure to represent the situation, identify the Zscore for the value of interest. 
Many software packages return the area to the left when given a Zscore, if you would like to calculate the area to the right, first find the area to the left and then subtract this amount from one. 

Probability falling within:
* 1 Standard Deviation from the mean: 68%
* 2 Standard Deviation from the mean: 95%
* 3 Standard Deviation from the mean: 99.7%
If a distribution is truely normal, the probability of being more than 4 STDs from the mean if 1/15,000. 5 & 6 STDs is 1/2000000 & 1/500000000 respectively.

### Geometric Distribution
**Bernoulli distribution** when a rial only has two possible outcomes (e.g. success/failure). If X is a random variable that takes value 1 with a probability of success p and o with a probability 1-p, then X is a Bernoulli random variable with a mean and standard deviation.
**Sample Proportion** the sample mean p_hat = #success/#trials
**Geometric Distribution** describes how many trials it takes to observe a success. the waiting time until success for independent and identical distributed bernoulli random variables. In general, the probabilities for a geometric distribution decrease exponentially fast. If the probability of a success in one trial is p and the probability of a failure is 1-p, then the probability of finding the first success in the nth trial is given by (1-p)^n-1*p  The mean (i.e. expected value), variance and standard deviation of this wait time are given by: mu=1/p, sigma^2=(1-p)/p^2, sigma = sqrt( sigma^2 )
It takes an average of 1/p trials to get a success under the geometric distribution.

### Binomial distribution.
The binomial distribution describes the number of successes in a fixed numbr of trials it describes the probability of having exacty k successes in N independent bernoulli trials with probability of a success p.
suppose the probability of a single trial being a success is p. Then the probability of observing exactly k successes in n independent trials is given by:
[ # of scenarios ]x P( single scenario ) ->
n!/(k!(n-k)!) x p^k(1-p)^(n-k)
The mean, variance, and standard deviation of the number of observed successes are: mu=np sigma^2=np(1-p) sigma = sqrt( sigma^2 )

**is it binomial?**

1. the trials are independent
2. the number of trials, n, is fixed.
3. Each trial outcome can be classified as a success or failure.
4. the probability of a success, p, is the same for each trial.

**Computing binomial probabilities**

1. check if the model is appropriate
2. identify n, p and k
3. use software/formulas to determine the probability
4. interpret the results

Normal approximation to the bnomial distribution. In some cases we may use the normal distribution as an easier and faster way to estimate binomial probabilities.  The binomial distribution with probability of success p is nearly normal when the sample size , is sufficintly large that np and n(1-p) are both at least 10. The approximate normal distribution has parameters corresponding to the mean and standard deviation of the binomial distribution. mu=np  sigma = sqrt( np(1-p))

The normal approximation breaks down on small intervals

### Negative Binomial Distribution: describes the probability of observing the kth success on the nth trial where all trials are independent

**Is it negative binomial?

1. the trials are independent
2. each trial outcome can be classified as a success or failure
3. the probability of a success (p) is the same for each trial.
4. the last trial must be a success

break down the probability into the sum of the disjoint probabilities:
=[ Number of possible sequences ] x P( Single sequence ):
(n-1)!/((k-1)!(n-k)!) x p^k(1-p)^(n-k)

**Binomial vs Negative Binomial**
In the binomial case, we typically have a fixed number of trials and instead cinsider the number of successes. In the negative binomial case, we examine how may trials it takes to observe a fixed number of successes and require that the last observation be a success

### Poisson distribution
The rate of a poisson distribution is the average number of occurances in a mostly fixed population per unit time. The only parameter in the Poisson distribution is the rate - or how many events we expect to observe. Using the rate, we can describe the probability of observing exactly k events in a single unit of time.
P(observed k events) = (lambda^k)(e^-lambda)/k!
where the mean and standard deviation are given by lambda and sqrt(lambda) respectively.
A random variable may follow a Poisson distribution if we are looking for the number of events, the population that generates such events is large, and the events occur independently of each other.

## Chapter 5 
## Point estimates and sampling variability

### Point estimates and sampling variability
**point estimate** it is useful to think of a point estimate as a random draw from a sampling distribution
**error**: sampling error and bias
**sampling error** how much an estimate will tend to vary from one sample to the next
**bias** systematic tendancy to over or under-estimate a variable
A bigger sample tends to provide a more precise estimate than a smaller sample

**Central Limit Theorem** when observations are independent and the sample size is sufficiently large, the sample proportion will tend to follow a normal distribution. In order for the central limit theorem to hold, the sample size is typically considered sufficiently large when np>= 10 and n(1-p)>=10, which i s called tthe success-failure condition. If the independence and success-failure conditions are both met, the central limit theorem can be applied and it is reasonable to model using a normal distribution.

**independent observations:** the most common way for observations to be considered independent is if they are from a simple random sample. a random process, a random sample, or random assignment (e.g. treatment/control groups)

### Confidence Intervals for a sample proportion
when stating an estimate for aproportion, it is better practice to provide a plausible range of values instead of supplying just the point estimate.
**confidence interval**: represents a range of plausible values where we are likely to find a parameter.
**95% confident** = point estimate +/- 1.96 * sqrt(p(1-p)/n). When the distribution of a point estimate qualifies for the Cental Limit Theorem and therefore closely follows a normal distribution, we can construct a 95% confidence interval.
**99% confident** = point estimate +/- 2..58 * sqrt(p(1-p)/n)
**Confidence Interval using any Confidence Level** = point estimate +/- zScore(confidence level)*SE
**Margin of Error** = zScore*SE

**Constructing a Confidence Interval for a Single Proportion**  

1. **Prepare** Identify p_hat and n, and determine what confidence level you wish to use.
2. **Check** Verify the conditions to ensure p_hat is nearly normal. For one-proportion confidence intervals, use p_hat in place of p to check the success-failure condition.
3. **Calculate** If the conditions hold, compute SE using p_hat, find zScore, and construct the interval.
4. **Conclude** Interpret the confidence interval in the context of the problem.

Interpreting confidence intervals: while it is useful to think about confidence intervals as a probability, the confidence level only quantifies how plausible it is that the parameter is in the given interval. Also, confidence intervals are only about the population parameter. A confidence interval says nothing about individial observations or point estimates. Aso, also, only applies to sampling error & does not take bias into account.

### Hypothesis Testing for a Proportion
**Null and Alternative Hypotheses**
**H_0**: represents a skeptical perspective or a claim to be tested
**H_A**: represents an alternative claim under consideration and is often represented by a range of possible parameter values. H_A generally represents a new or stronger perspective.

Even is we fail to reject the null hypothesis, we typically do not accept the null hypothesis as true.

**Double Negatives** In many statistial explanations, we use doube negatives. For instance, we might say that the null hypothesis is not implausible or we failed to reject the null hypothesis. Double negatives are used to communicate that while we are not rejecting a position, we are not saying it is correct.

**Decision Errors** 
**Type 1 Error** we reject the null hypothesis whem H_0 is actually true
**Type 2 Error** we fail to reject the null hypothesis when the alternative is true.

**Significance Level** indicates how often the data lead us to incorrectly reject H_0

### Formal Testing using -pvalues  
**p-values**: is the probability of observing data at least as favorable to the alternative hypothesis as our current data set, if the null hypothesis were true. Typically, a summary statistic of the data (e.g. sample proportion) is used to compute the p-value and evaluate the hypothesis.
**Checking Success-Failure & Computing SEp_hat for a hypothesis test** When using the p-value method to evaluate a hpothesis test, we check the conditions for p_hat and construct the SE using the null value p_0, instead of using the sample proportion. In a hypothesis test with a p-value, we are supposing the null hypothesis is true, which is a different mindset than whne we compute a confidence interval. This is why we use p_0 instead of p-hat when we check conditions and compute the SE in this context.
**Null Distribution** the sampling distribution under the null hypothesis
**Compare the p-value to alpha to evaluate H_0** When the p-value is less than the significance level, alha, reject H_0. We would report a conclusion that the data provide strong evidence supporting the alternative hypothesis. When the p-value is greater than alpha, do not reject H_0, and report that we do not have sufficient evidence to reject the null hypothesis. Describe the conclusion in the context of the data.

**Steps for Hypothesis Testing**  

1. **Prepare** Identify the parameterof interest, list hypotheses, identify the significance level, and identify p_hat and n.
2. **Check** Verify conditions to ensure p_hat is nearly normal under H_0. For one-proportion hypothesis tests, use the null value to check the success-failure condition.
3. **Calculate** If the conditions hold, compute the standard error, again using p_0, compute the Z-score, and identify the p-value.
4. **Conclude** Evaluate the hypothesis test by comparing the p-value to alpha, and provide a conclusion in the context of the data.

### Choosing a Significance Level

* If making a Type 1 Error is dangerous or especially costly, we should choose a small significance level to be cautious about rejecting the null hypothesis.
* If a Type 2 Error is relatively more dangerous or much more costly, then we might set a higher significance level.
* if the cost of data collection is small relative to the cost of a type2 error, then it makes sense to collect more data. This will reduce type2 errors without affecting type1 error rate.

**1-sided Hypothesis Testing**: we compute the p-value as the tail area in the direction of the alternative hypothesis only.


## Chapter 6
## Inference for Categorical Data

### Inference for a Single Proportion
How to choose an appropriate sample size when collecting data for single proportions.
**p_hat** sample proportion: assume normal if the sample observations are independent and is the sample size is sufficiently large. If these conditions are met, we can assume a **mean** (p) and a **standard error** SE=sqrt((p(1-p))/n) & **confidence interval** p_hat +/- z*xSE

**success-failure condition** We expect to see at least 10 successes and 10 failures in the sample. np>=10 & n(1-p)>=10.

### Hypothesis testing for a single proportion: once you've determines a one-proportion hypothesis test is the correct procedure, the are four steps to completing the test:  

1. **Prepare** Identify the parameters of interest, list hypotheses, identify the sigificance level and identify p_hat and n.
2. **Check** Verify conditions to ensure p_hat is nearly normal under H0. For one-proportion hypothesis tests, use the null value to check the success-failure condition.
3. **Calculate** If the condition holds, compute the standard error, again using the p0, compute the Zscore, and identify the p-value.
4. **Conclude** Evaluate the hypothesis test by comparing the p-value to alpha, and provide a conclusion in the context of the problem.

**Clopper-Pearson interval** use for a confidence interval when the success-failure condition isn't met.

### Choosing a sample size when estimating a proportion
need to choose a sample size large enough that the margin of error is sufficiently small so that the sample is useful
**margin of error** = z*( sqrt(p(1-p)/n))  solve for n
if you have an estimate of p, use that. if not, assume the worst and use p=05 since at this p, the margin of error is greatest.

### Difference of Two Proportions
Verify that the point estimate can be modeled using a normaldistribution, compute the estimate's standard error & apply inferential framework...

Conditions for the sampling distribution of p1-p2 to be normal  

1. **Independence Extended** The data ae independent within and between the two groups. Generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment.
2. **Success-failure Conditions** The success-failure condition holds for both groups, where we check successes and failures in each group seperately.
**Standard Error** SE = sqrt( p1(1-p1)/n1 + p2(1-p2)/n2 )
**Confidence Interval** CI = ( p1-p2 ) +/- z*SE

Hypothesis tests for the difference of two populations
**Pooled Proportion** When the null hypothesis is that the proportions are equal, use the pooled proportion to verify the success-failure condition and estimate the standard error:
p_pooled = (number of successes)/(number of cases) = (p1n1 + p2n2)/(n1+n2)

### Testing for goodness of fit using chi-square


## Chapter 7
## Inference for Numerical Data

### One sample means with the t-distribution

**Central Limit Theorem for the Sample Mean** when we collect sufficintly large sample of n independent observations from a population with mean mu and standard deviation sigma, the sample distribution of xbar will be nearly normal with Mean=mu and SE=sigma/sqrt(n)
**How to perform the Normality Check** Thre is no perfect way to check the normality condition, so instead we use two rules of thumb:  

1. **n<30** if the sample size n is less than 30 and there are no clear outliers in the data, then we typically assume the data came from a nearly normal distribution
2. **n>=30** if the sample size of n is greater than or equal to 30 and there are no particularly extreme outliers, then we typically assume the sampling distribution of xbar is nearly normal, even if the underlying distribution of individual observations is not.

In practice, we cannot directly calculate the SE for xbar since we do not know the population standard deviation sigma, so we estimate with the sample standard deviation: **SE = sigma/sqrt(n) ~~ sampleSD/sqrt(n)**

**t-distribution** used for inference calculations. has a thicker tail so thatobservations are more likely to fall beyond 2 SD from the mean; this helps correct for using the sampleSD in place of sigma. for large n, the t-distribution is nearly indistinguishable from the normal distribution.

**Degrees of Freedom** the degrees of freedom describes the shape of the t-distribution. the larger the degrees of freedom, the more closely the distribution approximates the normal model. When modeling xbar using the t-distribution, use df= n-1

#### One Sample t-confidence intervals
In the normal model, we use z and the SE to determine the width of a confidence interval. **CI = xbar +/- t_df*sampleSD/sqrt(n)**
Where xbar is the sample mean, t_df corresponds to the confidence level and degrees of freedom, and SE is the standard error as estimated by the sample.

**Confidence Interval for a Single Mean:**
Once you've determines a one-mean confidence interval would be helpful for an application there are four-steps to constructing the interval.  

1. **Prepare** Identify xbar, s, n and determine the confidence level you wish to use
2. **Check** Varify the conditions to ensure xbar is nearly normal.
3. **Calculate** If the conditions hold, compute SE, find the t_df and construct the interval


## Chapter 8
## Introduction to Liner Regression

### fitting a line, residuals, and correlation
**Linear regression** is the statistical method for fitting a line to data where the relationship between two variables, x and y, can be modeled by a straight line with some error.
y = B0 + B1*x + error, where x is the explanatory/predictor variable and y is the response.
**Residuals** are the leftover variation in the data after accounting for the model fit. A goal in picking the right regression fit is for residuals to be as small as possible. residual is the difference between the observed and the expected values.
**Correlation** the strength of a linear relationship. correlation takes values between -1 and 1.
R = 1/(n-1)*sum((x-xbar/Sdx)*(y-ybar/SDy)). Only when the relationship is perfectly linear is the correlation ever 1 or -1. If there is no appearant linear relationship between the variables, then the correlation will be near zero.

### **Least Squares Regression:** choose a line that minimizes the sum of the squared residuals
te least squares is 1) most common 2) widely supported by stats software & 3) squaring the residuals accounts for the fact that a residual twice as large as another residual is more than twice as bad.
Conditions for the least squares fit:

1. Linearity. data should show a linear trend
2. Nearly Normal Residuals. e.g. no extreme outliers
3. Constant variability. variability doesn't change as a function of x.
4. Independent observations.

Slope of least squares line can be estimated as : SDy/SDx * R = ratio of sample standard deviations times the correlation.
The point (Xmean,Ymean) is on the least squares line by definition.
The slope describes the estimated difference in the y variable if the explanatory variable x for a case happened to be one unit larger. The intercept describes the average outcome of y if x=0 and the linear model is valid all the way through 0, which in many applications is not the case.

**extrapolation** applying a model estimate to values outside of the realm of the original data.

**R^2** how closely the data cluster around a linear fit. described the amount of variation in the response that is explined by the least squares line.

### categorical predictors with two levels. 
the estimated intercept is the value of the response variable for the first category( i.e. the category corresponding to the indicator value 0). the estimated slope is the average change in the response variavle between the two categories.

### Types of Outliers in Linear Regression
which outliers are iportant and influential.
**Leverage** points that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with higher leverage.
**influential point** a high leverage points that influences the slope of the regression line.

### Inference fro Linear Regression